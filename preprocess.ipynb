{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train = pd.read_csv(\"train_stances.csv\")\n",
    "#train_body = pd.read_csv(\"train_bodies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"competition_test_stances.csv\")\n",
    "group = train.groupby('Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated\n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated\n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated\n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated\n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body = pd.read_csv(\"competition_test_bodies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Apple installing safes in-store to protect gol...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>El-Sisi denies claims he'll give Sinai land to...</td>\n",
       "      <td>1</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>Apple to keep gold Watch Editions in special i...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>Apple Stores to Keep Gold “Edition” Apple Watc...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3773</th>\n",
       "      <td>South Korean woman's hair 'eaten' by robot vac...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>18-Karat Gold Apple Watch Edition To Be Locked...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>Apple Watch Price, Release Date News: Special ...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>Apple plans to phase out Beats Music brand, re...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12418</th>\n",
       "      <td>Apple Stores will have custom-designed safes t...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12823</th>\n",
       "      <td>Polish Teenager Wakes Up During Brain Surgery,...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14989</th>\n",
       "      <td>CNN: Doctor Took Mid-Surgery Selfie with Uncon...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>Gold Apple Watch Will Be Stored In Special Saf...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19052</th>\n",
       "      <td>Teen wakes up during brain surgery, asks how i...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19929</th>\n",
       "      <td>Apple Stores to install safes to secure gold A...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20899</th>\n",
       "      <td>Apple Watch Edition: special safes to be insta...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20908</th>\n",
       "      <td>Apple Watches to be kept in safes</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21311</th>\n",
       "      <td>Apple Stores Are Reportedly Installing Custom ...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841</th>\n",
       "      <td>Gold Apple Watches to be stored in safes in Ap...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>NHL expansion to include Toronto, Quebec City,...</td>\n",
       "      <td>1</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  Body ID     Stance\n",
       "176    Apple installing safes in-store to protect gol...        1  unrelated\n",
       "1647   El-Sisi denies claims he'll give Sinai land to...        1      agree\n",
       "1932   Apple to keep gold Watch Editions in special i...        1  unrelated\n",
       "3625   Apple Stores to Keep Gold “Edition” Apple Watc...        1  unrelated\n",
       "3773   South Korean woman's hair 'eaten' by robot vac...        1  unrelated\n",
       "5037   18-Karat Gold Apple Watch Edition To Be Locked...        1  unrelated\n",
       "6755   Apple Watch Price, Release Date News: Special ...        1  unrelated\n",
       "9975   Apple plans to phase out Beats Music brand, re...        1  unrelated\n",
       "12418  Apple Stores will have custom-designed safes t...        1  unrelated\n",
       "12823  Polish Teenager Wakes Up During Brain Surgery,...        1  unrelated\n",
       "14989  CNN: Doctor Took Mid-Surgery Selfie with Uncon...        1  unrelated\n",
       "18157  Gold Apple Watch Will Be Stored In Special Saf...        1  unrelated\n",
       "19052  Teen wakes up during brain surgery, asks how i...        1  unrelated\n",
       "19929  Apple Stores to install safes to secure gold A...        1  unrelated\n",
       "20899  Apple Watch Edition: special safes to be insta...        1  unrelated\n",
       "20908                  Apple Watches to be kept in safes        1  unrelated\n",
       "21311  Apple Stores Are Reportedly Installing Custom ...        1  unrelated\n",
       "21841  Gold Apple Watches to be stored in safes in Ap...        1  unrelated\n",
       "22465  NHL expansion to include Toronto, Quebec City,...        1  unrelated"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.get_group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afghan woman kills 25 Taliban rebels to avenge her son’s murder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sort_values('Body ID')[30:40]#['Headline']#[30:40]\n",
    "train['Headline'][21704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Al-Sisi has denied Israeli reports stating tha...\n",
       "1      A bereaved Afghan mother took revenge on the T...\n",
       "2      CNBC is reporting Tesla has chosen Nevada as t...\n",
       "3      A 4-inch version of the iPhone 6 is said to be...\n",
       "4      GR editor’s Note\\n\\nThere are no reports in th...\n",
       "                             ...                        \n",
       "899    Congressional Republicans, evidently hoping th...\n",
       "900    Did Obamacare work?\\n\\nIt’s worth reflecting u...\n",
       "901    Millions may lose coverage next year if Congre...\n",
       "902    Come November, the grim trudge across the incr...\n",
       "903    Remember how much Republicans wanted to repeal...\n",
       "Name: articleBody, Length: 904, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body['articleBody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al-Sisi has denied Israeli reports stating tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CNBC is reporting Tesla has chosen Nevada as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A 4-inch version of the iPhone 6 is said to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>GR editor’s Note\\n\\nThere are no reports in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        1  Al-Sisi has denied Israeli reports stating tha...\n",
       "1        2  A bereaved Afghan mother took revenge on the T...\n",
       "2        3  CNBC is reporting Tesla has chosen Nevada as t...\n",
       "3       12  A 4-inch version of the iPhone 6 is said to be...\n",
       "4       19  GR editor’s Note\\n\\nThere are no reports in th..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class BaseTokenizer(object):\n",
    "    def process_text(self, text):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def process(self, texts):\n",
    "        for text in texts:\n",
    "            yield self.process_text(text)\n",
    "\n",
    "\n",
    "RE_PATTERNS = {\n",
    "    ' american ':\n",
    "        [\n",
    "            'amerikan'\n",
    "        ],\n",
    "\n",
    "    ' adolf ':\n",
    "        [\n",
    "            'adolf'\n",
    "        ],\n",
    "\n",
    "    ' hitler ':\n",
    "        [\n",
    "            'hitler'\n",
    "        ],\n",
    "\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' trans gender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homo sex ual':\n",
    "        [\n",
    "            'homosexual'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
    "                                                                                      'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            ' raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "}\n",
    "\n",
    "\n",
    "class PatternTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lower=True,stem = True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.']\", patterns=RE_PATTERNS,\n",
    "                 remove_repetitions=True):\n",
    "        self.lower = lower\n",
    "        self.stem = stem\n",
    "        self.patterns = patterns\n",
    "        self.initial_filters = initial_filters\n",
    "        self.remove_repetitions = remove_repetitions\n",
    "\n",
    "    def process_text(self, text):\n",
    "        x = self._preprocess(text)\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                x = re.sub(pat, target, x)\n",
    "        x = re.sub(r\"[^a-z' ]\", ' ', x)\n",
    "        return x.split()\n",
    "\n",
    "    def process_ds(self, ds):\n",
    "        ### ds = Data series\n",
    "        # lower\n",
    "        ds = copy.deepcopy(ds)\n",
    "        if self.lower:\n",
    "            ds = ds.str.lower()\n",
    "        \n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            ds = ds.str.replace(self.initial_filters, ' ')\n",
    "        # fuuuuck => fuck\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "            ds = ds.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                ds = ds.str.replace(pat, target)\n",
    "\n",
    "        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n",
    "        corpus = []\n",
    "        if self.stem:\n",
    "            for words in ds.str.split():\n",
    "                sentence = []\n",
    "                for word in words:\n",
    "                    sentence.append(WordNetLemmatizer().lemmatize(word,'v'))\n",
    "                corpus.append(sentence)\n",
    "        #return ds.str.split()\n",
    "        #ds = pd.Series(corpus)\n",
    "        ds = corpus\n",
    "        corpus = []\n",
    "        if self.stem:\n",
    "            for words in ds:\n",
    "                sentence = []\n",
    "                for word in words:\n",
    "                    sentence.append(WordNetLemmatizer().lemmatize(word,'n'))\n",
    "                corpus.append(sentence)\n",
    "        #return ds.str.split()\n",
    "        ds = pd.Series(corpus)\n",
    "        return ds\n",
    "    def _preprocess(self, text):\n",
    "        # lower\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        if self.stem:\n",
    "            text = WordNetLemmatizer().lemmatize(text,'v')\n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            text = re.sub(self.initial_filters, ' ', text)\n",
    "\n",
    "        # fuuuuck => fuck\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "            text = pattern.sub(r\"\\1\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "def main():\n",
    "    #train = pd.read_json(\"train_gold.json\",lines=True)\n",
    "    #test = pd.read_json(\"test_unlabeled.json\",lines=True)\n",
    "    train = pd.read_csv(\"competition_test_stances.csv\")\n",
    "    train_body = pd.read_csv(\"competition_test_bodies.csv\")\n",
    "\n",
    "    tokenizer = PatternTokenizer()\n",
    "    train[\"Headline\"] = tokenizer.process_ds(train[\"Headline\"]).str.join(sep=\" \")\n",
    "    train_body[\"articleBody\"] = tokenizer.process_ds(train_body[\"articleBody\"]).str.join(sep=\" \")\n",
    "    return train,train_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,body = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ferguson riot pregnant woman lose eye after co...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crazy conservative be sure a gitmo detainee ki...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a russian guy say his justin bieber ringtone s...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zombie cat bury kitty believe dead meow back t...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>argentina's president adopt boy to end werewol...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  ferguson riot pregnant woman lose eye after co...     2008  unrelated\n",
       "1  crazy conservative be sure a gitmo detainee ki...     1550  unrelated\n",
       "2  a russian guy say his justin bieber ringtone s...        2  unrelated\n",
       "3  zombie cat bury kitty believe dead meow back t...     1793  unrelated\n",
       "4  argentina's president adopt boy to end werewol...       37  unrelated"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [\"agree\",\"disagree\",\"discuss\",\"unrelated\"]\n",
    "#for i in range(len(train)):\n",
    "#    for j in list_classes:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat[0] == train['Stance'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "tt = np.zeros((len(train['Stance']),len(cat)))\n",
    "for i in range(len(train['Stance'])):\n",
    "    for j in range(len(cat)):\n",
    "        if cat[j] == train['Stance'][i]:\n",
    "            tt[i][j] = 1\n",
    "        else:\n",
    "            tt[i][j] = 0\n",
    "df = {}\n",
    "for j in range(len(tt[0])):        \n",
    "    df[cat[j]] = tt[:,j]\n",
    "df_ = pd.DataFrame(df)\n",
    "train_ = pd.concat([train['Body ID'],train['Headline'], df_], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.head()\n",
    "#train_.to_csv('train_stance_pre.csv',index=False)\n",
    "train_.to_csv('test_stance_pre.csv',index=False)\n",
    "body.to_csv('test_body_pre.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classlabel = train.groupby('Stance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classlabel.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body['articleBody'][body['Body ID']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_['Body ID']:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Headline</th>\n",
       "      <th>agree</th>\n",
       "      <th>disagree</th>\n",
       "      <th>discuss</th>\n",
       "      <th>unrelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>ferguson riot pregnant woman lose eye after co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550</td>\n",
       "      <td>crazy conservative be sure a gitmo detainee ki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a russian guy say his justin bieber ringtone s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>zombie cat bury kitty believe dead meow back t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>argentina's president adopt boy to end werewol...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                           Headline  agree  \\\n",
       "0     2008  ferguson riot pregnant woman lose eye after co...    0.0   \n",
       "1     1550  crazy conservative be sure a gitmo detainee ki...    0.0   \n",
       "2        2  a russian guy say his justin bieber ringtone s...    0.0   \n",
       "3     1793  zombie cat bury kitty believe dead meow back t...    0.0   \n",
       "4       37  argentina's president adopt boy to end werewol...    0.0   \n",
       "\n",
       "   disagree  discuss  unrelated  \n",
       "0       0.0      0.0        1.0  \n",
       "1       0.0      0.0        1.0  \n",
       "2       0.0      0.0        1.0  \n",
       "3       0.0      0.0        1.0  \n",
       "4       0.0      0.0        1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = pd.read_csv('test_stance_pre.csv')\n",
    "ttt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "words = ['gave','went','going','dating']\n",
    "for word in words:\n",
    "       print (word+\"-->\"+WordNetLemmatizer().lemmatize(word,'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize('flying','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
