{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_size = 50 # how big is each word vector\n",
    "max_features = 30000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "train = pd.read_csv(\"train_stance_pre.csv\")\n",
    "train_body = pd.read_csv(\"train_body_pre.csv\")\n",
    "test = pd.read_csv(\"test_stance_pre.csv\")\n",
    "test_body = pd.read_csv(\"test_body_pre.csv\")\n",
    "list_sentences_train = train[\"Headline\"].fillna(\"_na_\").values\n",
    "list_sentences_body = train_body[\"articleBody\"]\n",
    "list_sentences_test = test[\"Headline\"].fillna(\"_na_\").values\n",
    "list_sentences_body1 = test_body[\"articleBody\"]\n",
    "#subm = pd.read_csv(\"sample_submission.csv\")\n",
    "#cat = pd.read_json(\"categories.json\")\n",
    "list_classes = [\"agree\",\"disagree\",\"discuss\",\"unrelated\"]\n",
    "\n",
    "#list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "tokenizer.fit_on_texts(list(list_sentences_body))\n",
    "tokenizer.fit_on_texts(list(list_sentences_test))\n",
    "tokenizer.fit_on_texts(list(list_sentences_body1))\n",
    "list_tokenized_train_sent = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_train_body = tokenizer.texts_to_sequences(list_sentences_body)\n",
    "list_tokenized_test_sent = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "list_tokenized_test_body = tokenizer.texts_to_sequences(list_sentences_body1)\n",
    "X_t = pad_sequences(list_tokenized_train_sent, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_train_body, maxlen=300)\n",
    "X_tte = np.zeros((len(X_t),300),dtype = np.int32)\n",
    "for i in range(len(train['Body ID'])):\n",
    "    X_tte[i] = X_te[train_body['Body ID']==train['Body ID'][i]]\n",
    "train_text = X_t\n",
    "train_body = X_tte\n",
    "y_train = np.array(y,dtype=np.float32)\n",
    "y = test[list_classes].values\n",
    "X_t = pad_sequences(list_tokenized_test_sent, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test_body, maxlen=300)\n",
    "X_tte = np.zeros((len(X_t),300),dtype = np.int32)\n",
    "for i in range(len(test['Body ID'])):\n",
    "    X_tte[i] = X_te[test_body['Body ID']==test['Body ID'][i]]\n",
    "test_text = X_t\n",
    "test_body = X_tte\n",
    "y_test = np.array(y,dtype=np.float32)\n",
    "#[test_text,test_body],y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_3_idx = sum([np.where(y==1)[1] == 3])\n",
    "#label_2_idx = sum([np.where(y==1)[1] == 2])\n",
    "#label_1_idx = sum([np.where(y==1)[1] == 1])\n",
    "#label_0_idx = sum([np.where(y==1)[1] == 0])\n",
    "#label_0_idx = [idx  for idx,t in enumerate(label_0_idx) if t == 1]\n",
    "#label_1_idx = [idx  for idx,t in enumerate(label_1_idx) if t == 1]\n",
    "#label_2_idx = [idx  for idx,t in enumerate(label_2_idx) if t == 1]\n",
    "#label_3_idx = [idx  for idx,t in enumerate(label_3_idx) if t == 1]\n",
    "#np.sum(label_1_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]\n",
    "\n",
    "def score_submission(gold_labels, test_labels):\n",
    "    score = 0.0\n",
    "    cm = [[0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0],\n",
    "          [0, 0, 0, 0]]\n",
    "\n",
    "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
    "        g_stance, t_stance = g, t\n",
    "        if g_stance == t_stance:\n",
    "            score += 0.25\n",
    "            if g_stance != 'unrelated':\n",
    "                score += 0.50\n",
    "        if g_stance in RELATED and t_stance in RELATED:\n",
    "            score += 0.25\n",
    "\n",
    "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
    "\n",
    "    return score, cm\n",
    "\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    lines = []\n",
    "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
    "    line_len = len(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "    lines.append(header)\n",
    "    lines.append(\"-\"*line_len)\n",
    "\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for i, row in enumerate(cm):\n",
    "        hit += row[i]\n",
    "        total += sum(row)\n",
    "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
    "                                                                   *row))\n",
    "        lines.append(\"-\"*line_len)\n",
    "    print('\\n'.join(lines))\n",
    "\n",
    "\n",
    "def report_score(actual,predicted):\n",
    "    score,cm = score_submission(actual,predicted)\n",
    "    best_score, _ = score_submission(actual,actual)\n",
    "\n",
    "    print_confusion_matrix(cm)\n",
    "    print(\"Score: \" +str(score) + \" out of \" + str(best_score) + \"\\t(\"+str(score*100/best_score) + \"%)\")\n",
    "    return score*100/best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {'unrelated': 3 , 'agree':0, 'disagree':1, 'discuss':2}\n",
    "def evaluate_answer(model,model_inp, true):\n",
    "    inv_category_dict = {3:'unrelated', 0: 'agree', 1: 'disagree', 2: 'discuss'}\n",
    "    predicted = model.predict(model_inp)\n",
    "    predicted = np.argmax(predicted,axis = 1)\n",
    "    t = np.argmax(true,axis = 1)\n",
    "    ground = list()\n",
    "    pred = list()\n",
    "    for i in predicted:\n",
    "        pred.append(inv_category_dict[i])\n",
    "    for i in t:\n",
    "        ground.append(inv_category_dict[i])\n",
    "    report_score(ground, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "embed_size = 300 # how big is each word vector\n",
    "#max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = min(len(tokenizer.word_index),30000)\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(max_features, embed_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "def loadGloveModel():\n",
    "    print(\"Loading Glove Model\")\n",
    "\n",
    "    f = open('glove.6B.100d.txt','r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "def loadGoogleModel():\n",
    "    w = models.KeyedVectors.load_word2vec_format(\n",
    "    './GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    return w\n",
    "def get_embed(word_idx,max_features,embed_dim):\n",
    "    #glove = loadGloveModel()\n",
    "    glove = loadGoogleModel()\n",
    "    #all_embs = np.stack(glove.to_numpy())\n",
    "    #all_embs = np.stack(glove.values())\n",
    "    #emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean = 0\n",
    "    emb_std = 0.6\n",
    "    word_index = word_idx\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features+1, embed_dim))\n",
    "    #print(\"go into model\")\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        try: \n",
    "            embedding_vector = glove[word]\n",
    "        except:\n",
    "            None\n",
    "            #print('skip')\n",
    "            #embedding_vector = np.random.normal(scale=0.6, size=(100, ))\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "embedding_matrix = get_embed(tokenizer.word_index,max_features,embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "#     Usage\n",
    "#     y_train = create_labels(train_dataset)\n",
    "#     y_test = create_labels(test_dataset)\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_head = 100\n",
    "max_body =300\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import SpatialDropout1D\n",
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(tokenizer.word_index)+1 ,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "head_1d = Conv1D(200,3,padding='same',activation='relu')\n",
    "body_1d = Conv1D(200,3,padding='same',activation='relu')\n",
    "# we use max pooling:\n",
    "shared_lstm = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='head_lstm'))\n",
    "head_conv = head_1d(head_embed)\n",
    "body_conv = body_1d(body_embed)\n",
    "head_conv = SpatialDropout1D(0.35)(head_conv)\n",
    "body_conv = SpatialDropout1D(0.35)(body_conv)\n",
    "head_lstm = shared_lstm(head_conv)\n",
    "body_lstm = shared_lstm(body_conv)\n",
    "dot_layer = dot([head_lstm,body_lstm],axes = 1, normalize=True)\n",
    "\n",
    "conc = concatenate([head_lstm,body_lstm,dot_layer])\n",
    "#print(conc)\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "print(model.summary())\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "#visualize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='test.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44974 samples, validate on 4998 samples\n",
      "Epoch 1/5\n",
      "44974/44974 [==============================] - 2072s 46ms/step - loss: 0.2750 - acc: 0.8852 - val_loss: 0.1884 - val_acc: 0.9258\n",
      "Epoch 2/5\n",
      "22592/44974 [==============>...............] - ETA: 17:07 - loss: 0.2463 - acc: 0.9006"
     ]
    }
   ],
   "source": [
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "#for i in range(5):\n",
    "history = model.fit([train_text,train_body],[y_train],validation_data = ([test_text,test_body],y_test),epochs=5, batch_size=64,verbose = True)\n",
    "#history = model.fit([train_text,train_body],[y_train],epochs=1, batch_size=64,verbose = True)\n",
    "evaluate_answer(model,[test_text,test_body],y_test)\n",
    "#evaluate_answer(model,[train_text,train_body],y_train)\n",
    "#print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.savefig(\"acc.png\")\n",
    "plt.show()\n",
    "# summarize history for loss \n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.savefig(\"loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text,test_text,train_body,test_body,y_train,y_test = train_test_split(train_text,train_body,y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answer(model,[test_text,test_body],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_answer(model,[train_text,train_body],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
